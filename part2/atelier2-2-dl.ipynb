{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"used device :\", device)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:51:47.443075Z","iopub.execute_input":"2025-12-11T17:51:47.443703Z","iopub.status.idle":"2025-12-11T17:51:51.918983Z","shell.execute_reply.started":"2025-12-11T17:51:47.443677Z","shell.execute_reply":"2025-12-11T17:51:51.917981Z"}},"outputs":[{"name":"stdout","text":"used device : cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def load_mnist_images(path):\n    with open(path, 'rb') as f:\n        data = np.frombuffer(f.read(), np.uint8, offset=16)\n        data = data.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0\n    return data\n\ndef load_mnist_labels(path):\n    with open(path, 'rb') as f:\n        labels = np.frombuffer(f.read(), np.uint8, offset=8)\n    return labels\n\ntrain_images = load_mnist_images(\"/kaggle/input/train-images.idx3-ubyte\")\ntrain_labels = load_mnist_labels(\"/kaggle/input/train-labels.idx1-ubyte\")\n\ntest_images = load_mnist_images(\"/kaggle/input/t10k-images.idx3-ubyte\")\ntest_labels = load_mnist_labels(\"/kaggle/input/t10k-labels.idx1-ubyte\")\n\nprint(\"Train images :\", train_images.shape)\nprint(\"Test images  :\", test_images.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:52:05.473353Z","iopub.execute_input":"2025-12-11T17:52:05.473752Z","iopub.status.idle":"2025-12-11T17:52:06.255906Z","shell.execute_reply.started":"2025-12-11T17:52:05.473730Z","shell.execute_reply":"2025-12-11T17:52:06.254915Z"}},"outputs":[{"name":"stdout","text":"Train images : (60000, 1, 28, 28)\nTest images  : (10000, 1, 28, 28)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class MnistDataset(Dataset):\n    def __init__(self, images, labels):\n        self.images = torch.tensor(images, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]\n\ntrain_dataset = MnistDataset(train_images, train_labels)\ntest_dataset  = MnistDataset(test_images, test_labels)\n\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:52:26.333593Z","iopub.execute_input":"2025-12-11T17:52:26.334159Z","iopub.status.idle":"2025-12-11T17:52:26.443393Z","shell.execute_reply.started":"2025-12-11T17:52:26.334133Z","shell.execute_reply":"2025-12-11T17:52:26.442773Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size=28, patch_size=7, emb_dim=128):\n        super().__init__()\n        self.patch_size = patch_size\n\n        self.proj = nn.Conv2d(\n            in_channels=1,\n            out_channels=emb_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n        self.num_patches = (img_size // patch_size) ** 2\n\n    def forward(self, x):\n        x = self.proj(x)\n        x = x.flatten(2)\n        x = x.transpose(1, 2)\n        return x\n\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, emb_dim=128, num_heads=4):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = emb_dim // num_heads\n\n        self.query = nn.Linear(emb_dim, emb_dim)\n        self.key   = nn.Linear(emb_dim, emb_dim)\n        self.value = nn.Linear(emb_dim, emb_dim)\n\n        self.out = nn.Linear(emb_dim, emb_dim)\n\n    def forward(self, x):\n        B, N, D = x.shape\n        H = self.num_heads\n\n        q = self.query(x).reshape(B, N, H, D//H).transpose(1, 2)\n        k = self.key(x).reshape(B, N, H, D//H).transpose(1, 2)\n        v = self.value(x).reshape(B, N, H, D//H).transpose(1, 2)\n\n        scores = (q @ k.transpose(-2, -1)) / (D**0.5)\n        att = torch.softmax(scores, dim=-1)\n        out = att @ v\n\n        out = out.transpose(1, 2).reshape(B, N, D)\n        return self.out(out)\n\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, emb_dim=128, num_heads=4, mlp_ratio=2):\n        super().__init__()\n        self.ln1 = nn.LayerNorm(emb_dim)\n        self.mhsa = MultiHeadSelfAttention(emb_dim, num_heads)\n\n        self.ln2 = nn.LayerNorm(emb_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(emb_dim, emb_dim * mlp_ratio),\n            nn.ReLU(),\n            nn.Linear(emb_dim * mlp_ratio, emb_dim)\n        )\n\n    def forward(self, x):\n        x = x + self.mhsa(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n\nclass ViT(nn.Module):\n    def __init__(self, img_size=28, patch_size=7, emb_dim=128, depth=4, num_heads=4, num_classes=10):\n        super().__init__()\n\n        self.patch_embed = PatchEmbedding(img_size, patch_size, emb_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, emb_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, emb_dim))\n\n        self.encoder = nn.Sequential(\n            *[TransformerEncoderBlock(emb_dim, num_heads) for _ in range(depth)]\n        )\n\n        self.head = nn.Linear(emb_dim, num_classes)\n\n    def forward(self, x):\n        B = x.shape[0]\n        x = self.patch_embed(x)\n\n        cls = self.cls_token.expand(B, -1, -1)\n        x = torch.cat([cls, x], dim=1)\n\n        x = x + self.pos_embed\n\n        x = self.encoder(x)\n\n        cls_output = x[:, 0]\n        return self.head(cls_output)\n\n\nmodel_vit = ViT().to(device)\nprint(model_vit)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:52:30.575939Z","iopub.execute_input":"2025-12-11T17:52:30.576515Z","iopub.status.idle":"2025-12-11T17:52:30.802033Z","shell.execute_reply.started":"2025-12-11T17:52:30.576491Z","shell.execute_reply":"2025-12-11T17:52:30.801329Z"}},"outputs":[{"name":"stdout","text":"ViT(\n  (patch_embed): PatchEmbedding(\n    (proj): Conv2d(1, 128, kernel_size=(7, 7), stride=(7, 7))\n  )\n  (encoder): Sequential(\n    (0): TransformerEncoderBlock(\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mhsa): MultiHeadSelfAttention(\n        (query): Linear(in_features=128, out_features=128, bias=True)\n        (key): Linear(in_features=128, out_features=128, bias=True)\n        (value): Linear(in_features=128, out_features=128, bias=True)\n        (out): Linear(in_features=128, out_features=128, bias=True)\n      )\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=128, bias=True)\n      )\n    )\n    (1): TransformerEncoderBlock(\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mhsa): MultiHeadSelfAttention(\n        (query): Linear(in_features=128, out_features=128, bias=True)\n        (key): Linear(in_features=128, out_features=128, bias=True)\n        (value): Linear(in_features=128, out_features=128, bias=True)\n        (out): Linear(in_features=128, out_features=128, bias=True)\n      )\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=128, bias=True)\n      )\n    )\n    (2): TransformerEncoderBlock(\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mhsa): MultiHeadSelfAttention(\n        (query): Linear(in_features=128, out_features=128, bias=True)\n        (key): Linear(in_features=128, out_features=128, bias=True)\n        (value): Linear(in_features=128, out_features=128, bias=True)\n        (out): Linear(in_features=128, out_features=128, bias=True)\n      )\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=128, bias=True)\n      )\n    )\n    (3): TransformerEncoderBlock(\n      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mhsa): MultiHeadSelfAttention(\n        (query): Linear(in_features=128, out_features=128, bias=True)\n        (key): Linear(in_features=128, out_features=128, bias=True)\n        (value): Linear(in_features=128, out_features=128, bias=True)\n        (out): Linear(in_features=128, out_features=128, bias=True)\n      )\n      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=128, out_features=256, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=256, out_features=128, bias=True)\n      )\n    )\n  )\n  (head): Linear(in_features=128, out_features=10, bias=True)\n)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model_vit.parameters(), lr=0.001)\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    model_vit.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model_vit(images)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * images.size(0)\n\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    print(f\"Epoch {epoch+1}/{num_epochs} | Loss={total_loss/total:.4f} | Acc={(correct/total)*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:52:35.003185Z","iopub.execute_input":"2025-12-11T17:52:35.003958Z","iopub.status.idle":"2025-12-11T17:53:32.202340Z","shell.execute_reply.started":"2025-12-11T17:52:35.003931Z","shell.execute_reply":"2025-12-11T17:53:32.201585Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 | Loss=0.5353 | Acc=82.27%\nEpoch 2/10 | Loss=0.1514 | Acc=95.27%\nEpoch 3/10 | Loss=0.1096 | Acc=96.60%\nEpoch 4/10 | Loss=0.0887 | Acc=97.17%\nEpoch 5/10 | Loss=0.0765 | Acc=97.51%\nEpoch 6/10 | Loss=0.0689 | Acc=97.82%\nEpoch 7/10 | Loss=0.0615 | Acc=98.00%\nEpoch 8/10 | Loss=0.0566 | Acc=98.13%\nEpoch 9/10 | Loss=0.0509 | Acc=98.36%\nEpoch 10/10 | Loss=0.0521 | Acc=98.30%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model_vit.eval()\n\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        outputs = model_vit(images)\n        _, preds = torch.max(outputs, 1)\n\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\naccuracy = (np.array(all_preds) == np.array(all_labels)).mean() * 100\nf1 = f1_score(all_labels, all_preds, average=\"macro\")\n\nprint(\"Accuracy ViT :\", round(accuracy, 2), \"%\")\nprint(\"F1-score     :\", round(f1, 4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T17:55:05.640310Z","iopub.execute_input":"2025-12-11T17:55:05.641032Z","iopub.status.idle":"2025-12-11T17:55:06.038714Z","shell.execute_reply.started":"2025-12-11T17:55:05.641009Z","shell.execute_reply":"2025-12-11T17:55:06.038075Z"}},"outputs":[{"name":"stdout","text":"Accuracy ViT : 97.85 %\nF1-score     : 0.9783\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}